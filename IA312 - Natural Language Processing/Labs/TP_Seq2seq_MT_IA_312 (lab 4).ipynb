{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lab made by Melek GHOUMA / Mohamed Aymen BOUYAHIA"
      ],
      "metadata": {
        "id": "Lric7TlkSbri"
      },
      "id": "Lric7TlkSbri"
    },
    {
      "cell_type": "markdown",
      "id": "6840d517",
      "metadata": {
        "id": "6840d517"
      },
      "source": [
        "# Machine Translation with Seq2seq models via Pytorch\n",
        "\n",
        "The goal of this lab are to:\n",
        "- Familiarize yourself with the task of **Machine Translation (MT)**\n",
        "- Implement a basic **recurrent sequence-to-sequence** model in Pytorch\n",
        "- Train the model on a very simple English-French MT dataset\n",
        "- Implement an **attention** module into the model and visualize its results\n",
        "\n",
        "We will in this lab focus on the model and leave aside what are normally very important aspects of Machine Learning methodology: in particular, we won't use validation and test data to search for hyperparameter search and performance evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c5a0974",
      "metadata": {
        "id": "6c5a0974"
      },
      "outputs": [],
      "source": [
        "# General stuff\n",
        "from io import open\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Nice printing\n",
        "from pprint import pprint\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Which device to use ?\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fa8004",
      "metadata": {
        "id": "a9fa8004"
      },
      "source": [
        "### I Dataset and pre-processing\n",
        "\n",
        "We're going to work with data from the **tatoeba** website. This website proposes human-made translations for many (relatively) simple sentences, with sometimes several possible translations for one sentence.\n",
        "Pre-processed versions of the *tatoeba dataset* can be found on this [website](https://www.manythings.org/anki/). On the moodle, you can find the 'English $\\rightarrow$ French' data already cleaned, but you are free to use any other language you would prefer.  \n",
        "\n",
        "\n",
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>\n",
        "            \n",
        "We will define these as global variables - for convenience. Given what was said in class and how they are employed in this lab, explain briefly what each one is used for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d2ecd3",
      "metadata": {
        "id": "30d2ecd3"
      },
      "outputs": [],
      "source": [
        "# Some global variables\n",
        "PAD_TOKEN = 0\n",
        "SOS_TOKEN = 1\n",
        "EOS_TOKEN = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fbcf156",
      "metadata": {
        "id": "3fbcf156"
      },
      "outputs": [],
      "source": [
        "# PAD_TOKEN : used to fill in sequences to ensure they all have the same length when batched together\n",
        "# SOS_TOKEN : start-of-sequence token\n",
        "# EOS_TOKEN : end-of-sequence token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280f96ce",
      "metadata": {
        "id": "280f96ce"
      },
      "source": [
        "From the previous pytorch lab, we know we will require to define some parameters. We can already choose the maximum length of sequences, the size of our batches, and the internal dimension used by our model. Note that the length of sequence is rather short in this data - you can take a look at the histogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81270d1e",
      "metadata": {
        "id": "81270d1e"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "max_length = 10\n",
        "batch_size = 32\n",
        "hidden_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfb59d36",
      "metadata": {
        "id": "bfb59d36"
      },
      "outputs": [],
      "source": [
        "# Read the file and split into lines\n",
        "parallel = open('fra.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba28e54b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba28e54b",
        "outputId": "c837d983-d5c2-4895-fd63-9fd0b8977e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Go.\\tVa !',\n",
            " 'Run!\\tCours\\u202f!',\n",
            " 'Run!\\tCourez\\u202f!',\n",
            " 'Wow!\\tÇa alors\\u202f!',\n",
            " 'Fire!\\tAu feu !']\n"
          ]
        }
      ],
      "source": [
        "# Data looks like this\n",
        "pprint(parallel[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8de7fd",
      "metadata": {
        "id": "ff8de7fd"
      },
      "source": [
        "We will need to clean this up. Use the regular expression package ```re``` to remove any non letter character. Be careful, though, with French, you need to keep the accents. We will then organize the data into pairs, as is usual in MT.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f5e8a6d",
      "metadata": {
        "id": "2f5e8a6d"
      },
      "outputs": [],
      "source": [
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    # Lowercase\n",
        "    s = s.lower()\n",
        "    # Trim, and remove non-letter characters\n",
        "    s = re.sub(r\"[^a-zàâçéèêëîïôûùüÿñ\\s']\", '', s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(parallel[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjmxaZTdkeWN",
        "outputId": "7f25b991-d31e-47e1-8c47-9e458e7a2eda"
      },
      "id": "NjmxaZTdkeWN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Go.\\tVa !',\n",
            " 'Run!\\tCours\\u202f!',\n",
            " 'Run!\\tCourez\\u202f!',\n",
            " 'Wow!\\tÇa alors\\u202f!',\n",
            " 'Fire!\\tAu feu !']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16663844",
      "metadata": {
        "id": "16663844"
      },
      "outputs": [],
      "source": [
        "# Split every line into pairs and normalize\n",
        "pairs = [[normalizeString(s) for s in l.split('\\t')] for l in parallel]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49bd29eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49bd29eb",
        "outputId": "23c5a647-5c3a-4e3c-b265-d510dee9c489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['go', 'va '],\n",
            " ['run', 'cours\\u202f'],\n",
            " ['run', 'courez\\u202f'],\n",
            " ['wow', 'ça alors\\u202f'],\n",
            " ['fire', 'au feu ']]\n"
          ]
        }
      ],
      "source": [
        "pprint(pairs[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da162c2",
      "metadata": {
        "id": "7da162c2"
      },
      "source": [
        "Begin with implementing a class ```Vocab``` that will accumulate counts and indexes of words into language-specific dictionnaries. In this case, we would like the vocabulary to be built on the fly, to work well with the format of our data (parallel sentences from both languages).\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f09ee9",
      "metadata": {
        "id": "01f09ee9"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.word2count = {}\n",
        "        self.word2idx = {\"SOS\": SOS_TOKEN, \"EOS\": EOS_TOKEN}\n",
        "        self.idx2word = {SOS_TOKEN: \"SOS\", EOS_TOKEN: \"EOS\"}\n",
        "\n",
        "    # Implemented assuming we will process lines one by one, easier given the format of our data\n",
        "    def addSent(self, sent):\n",
        "        for word in sent.split(' '):\n",
        "          self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "      if word not in self.word2idx.keys():\n",
        "        last_pos = self.__len__()\n",
        "        self.word2idx[word] = last_pos\n",
        "        self.word2count[word] = 1\n",
        "        self.idx2word[last_pos] = word\n",
        "      else:\n",
        "        self.word2count[word] += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daa59369",
      "metadata": {
        "id": "daa59369"
      },
      "source": [
        "Then, create a function ```tensorFromSentence``` that will take an untokenized sentence (hence, a string), a ```Vocab``` object, and the ```max_length``` parameter as inputs, and return a ```LongTensor``` representing the sequence of indexes.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e6cb648",
      "metadata": {
        "id": "3e6cb648"
      },
      "outputs": [],
      "source": [
        "def tensorFromSentence(sent, vocab, max_length):\n",
        "    indexes = [vocab.word2idx[\"SOS\"]]\n",
        "    for word in sent.split(' '):\n",
        "        if word in vocab.word2idx:\n",
        "            indexes.append(vocab.word2idx[word])\n",
        "    indexes.append(vocab.word2idx[\"EOS\"])\n",
        "    return torch.tensor(indexes, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b7045aa",
      "metadata": {
        "id": "6b7045aa"
      },
      "source": [
        "Finally, complete this ```TranslationDataset``` class inheriting from ```Dataset```. It should, from the list of parallel sentences:\n",
        "- Apply an optional filter to possibly reduce the dataset size and complexity,\n",
        "- Instantiate and build ```Vocab``` objects for both languages,\n",
        "- Create two lists containing ```LongTensor``` objects for each language,\n",
        "- Group them into two tensors of the appropriate size with ```pad_sequence```.\n",
        "\n",
        "You should note that, depending on the ordering of the pairs, one language will be the **source**, and the other will be the **target** of our model. In this case, English is the source and French the target.\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70bd228",
      "metadata": {
        "id": "b70bd228"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, parallel_data, max_length = 10, filter_target_prefixes = None):\n",
        "        # We will select some subset on the data to avoid having too much\n",
        "        self.pairs = self.filterData(parallel_data, filter_target_prefixes)\n",
        "\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Creating both vocabularies\n",
        "        self.input_lang = Vocab()\n",
        "        self.output_lang = Vocab()\n",
        "\n",
        "        # Filling both vocabularies\n",
        "        for pair in self.pairs:\n",
        "            self.input_lang.addSent(pair[0])\n",
        "            self.output_lang.addSent(pair[1])\n",
        "\n",
        "        # Lists of tensors to be created\n",
        "        self.tensor_inputs = [tensorFromSentence(pair[0], self.input_lang, self.max_length) for pair in self.pairs]\n",
        "        self.tensor_outputs = [tensorFromSentence(pair[1], self.output_lang, self.max_length) for pair in self.pairs]\n",
        "        print(\"The tensor inputs is \\t\")\n",
        "        print(self.tensor_inputs[0])\n",
        "\n",
        "        # Put them all at the same size with pad_sequence\n",
        "        self.tensor_inputs = pad_sequence(self.tensor_inputs, padding_value=PAD_TOKEN, batch_first=True)\n",
        "        self.tensor_outputs = pad_sequence(self.tensor_outputs, padding_value=PAD_TOKEN, batch_first=True)\n",
        "        print(\"The tensor inputs is \\t\")\n",
        "        print(self.tensor_inputs[0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # The iterator just gets one particular example\n",
        "        # The dataloader will take care of the shuffling and batching\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        return self.tensor_inputs[idx], self.tensor_outputs[idx]\n",
        "\n",
        "    def filterPair(self, pair, prefixes):\n",
        "        return pair[0].startswith(prefixes)\n",
        "\n",
        "    def filterData(self, pairs, filter_target_prefixes):\n",
        "        if filter_target_prefixes is not None:\n",
        "            return [pair for pair in pairs if self.filterPair(pair, filter_target_prefixes)]\n",
        "        else:\n",
        "            return pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c0649e",
      "metadata": {
        "id": "e6c0649e"
      },
      "source": [
        "Create a ```TranslationDataset``` from our data, with no filter, and look at its size, and the sizes of the vocabularies. What could be a problem here ?\n",
        "\n",
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TranslationDataset from our data with no filter\n",
        "translationDataset = TranslationDataset(pairs, max_length=max_length)\n",
        "print(\"Observing the sizes of the vocabularies\\n\")\n",
        "print(\"The size of the source vocabulary is: {}\\n\".format(len(translationDataset.input_lang)))\n",
        "print(\"The size of the target vocabulary is: {}\\n\".format(len(translationDataset.output_lang)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ0jTdg0C1dq",
        "outputId": "7cb64720-bef8-4a06-d08c-b4caf83ea251"
      },
      "id": "YQ0jTdg0C1dq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensor inputs is \t\n",
            "tensor([1, 2, 2])\n",
            "The tensor inputs is \t\n",
            "tensor([1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0])\n",
            "Observing the sizes of the vocabularies\n",
            "\n",
            "The size of the source vocabulary is: 13653\n",
            "\n",
            "The size of the target vocabulary is: 29439\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We note that the target vocabulary (French) is double the size of the source vocabulary (English). This discrepancy could lead to potential issues in the model complexity, as the model must choose from a substantially broader vocabulary of possible words. Consequently, in the final layer, where a softmax function is applied across the vocabulary, it must manage a significantly larger class set.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IntWvAdmEUmE"
      },
      "id": "IntWvAdmEUmE"
    },
    {
      "cell_type": "markdown",
      "id": "dc6b413c",
      "metadata": {
        "id": "dc6b413c"
      },
      "source": [
        "We will now use a filter: we will only consider pairs of sentences which English begins with chains of characters from the ```prefixes``` set. Create the dataset with this filter. Look at the sizes involved. Create a dataloader with the previously defined ```batch_size```.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ef5801",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8ef5801",
        "outputId": "fdcf9916-07e3-4bef-bb8b-87c57aa162f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensor inputs is \t\n",
            "tensor([1, 2, 3, 4, 2])\n",
            "The tensor inputs is \t\n",
            "tensor([1, 2, 3, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "# Consider only the sentences beginning with these\n",
        "prefixes = (\"i am \", \"i m \",\n",
        "            \"he is\", \"he s \",\n",
        "            \"she is\", \"she s \",\n",
        "            \"you are\", \"you re \",\n",
        "            \"we are\", \"we re \",\n",
        "            \"they are\", \"they re \")\n",
        "\n",
        "# New dataset:\n",
        "new_dataset = TranslationDataset(pairs, max_length=max_length, filter_target_prefixes=prefixes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb041242",
      "metadata": {
        "id": "bb041242"
      },
      "outputs": [],
      "source": [
        "# Creating the dataloader\n",
        "training_dataloader = DataLoader(new_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14f35763",
      "metadata": {
        "id": "14f35763"
      },
      "source": [
        "### II - Sequence to sequence architecture and training\n",
        "\n",
        "We will now create two pytorch objects, which will inherit from ```Module```: the ```EncoderRNN``` and the ```DecoderRNN``` classes. Both are based on RNNs; we will use the lighter ```GRU``` (gated recurrent unit) recurrent layer.\n",
        "While we won't check it with validation data, we should try to avoid overfitting with ```Dropout```."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61234a32",
      "metadata": {
        "id": "61234a32"
      },
      "source": [
        "Begin by completing the **encoder**. It uses an ```Embedding``` layer, which has as many vectors as the size of the **source** vocabularies, plus the ```GRU```. Both embeddings and the recurrent layer use dimension ```hidden_size```. It should output two things:\n",
        "- A sequence of vectors, corresponding to the representations of each input word that has gone through the encoder,\n",
        "- The last hidden state used by the GRU of the encoder.\n",
        "\n",
        "**Important**: with our first decoder, we will only use the **last hidden state**. However, we can still add the sequence of representations to the outputs, as we will need it for the *attention module* later.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef96860",
      "metadata": {
        "id": "0ef96860"
      },
      "outputs": [],
      "source": [
        "# Create encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f1e1ac",
      "metadata": {
        "id": "28f1e1ac"
      },
      "source": [
        "Next, you will need to complete the **decoder**. Besides the ```Embedding``` (for the **target** language) and ```GRU```, it needs an additional layer: a ```Linear``` layer to obtain output scores for the next word to be predicted.\n",
        "The ```forward``` function is however a little more complicated: we will need it to be able to re-use what was predicted at the previous step during inference. Therefore, we will use the old-fashioned way: a **loop**. To summarize, we will:\n",
        "- Create an empty tensor containing only the first token of the output sequence (*which is ?*) with ```torch.empty```.\n",
        "- If we are in training mode, we can fill out that tensor with what we know to be the rest of the output sequence, make it go through the recurrent layer, and obtain scores.\n",
        "- If we are in inference mode, we need to make a prediction at each step to re-insert the corresponding index as input afterwards. We can use the ```topk``` method to get the best index directly ! **Important:** use the ```detach()``` method to cut this from the computational graph.  \n",
        "\n",
        "In both cases, we loop through the sequence and apply the same operations, which are in ```forward_step```. We return the log-probabilities of prediction at each step.\n",
        "\n",
        "**Important:** Again, we also return an empty placeholder variable which we will later use for attention.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f67a9b50",
      "metadata": {
        "id": "f67a9b50"
      },
      "outputs": [],
      "source": [
        "# Create decoder\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, max_length, target_tensor=None):\n",
        "        # We build the  decoder the old school way:\n",
        "        # As we will need to loop through the decoder for inference, let's use the same structure for training\n",
        "        # We will process one input and predict one output at the time, with a method implementing the recurrent step: \"foward_step\"\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        # Create the input to the decoder: which token is it ? Put it in \"fill_\"\n",
        "        # Which shape should it be ?\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_TOKEN)\n",
        "        # Where does the first hidden state come from ?\n",
        "        decoder_hidden = encoder_hidden\n",
        "        # We'll keep the output in a list\n",
        "        decoder_outputs = []\n",
        "\n",
        "        # Looping on the output sequence\n",
        "        for i in range(max_length):\n",
        "            # Apply the forward_step function ...\n",
        "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "            # and keep the output\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            # We are in training mode: we know the target\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                # Which shape do we need ?\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
        "\n",
        "            # We are doing inference, we need to predict the next word and re-use it as input\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                # Use the topk function to get the best index\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                # Very important: to be re-used as input, detach from computational graph\n",
        "                # Which shape do we need ?\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "\n",
        "        # Concatenate outputs on the second dimension (length of the sequence)\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        # Apply log_softmax\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        # We return `None` for consistency in the training loop - it will be used for attention later\n",
        "        return decoder_outputs, decoder_hidden, None\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        # Get your input through embedding, an activation function, the recurrent layer, and the output layer\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f0d850",
      "metadata": {
        "id": "58f0d850"
      },
      "source": [
        "Create an instance of one ```EncoderRNN``` and one ```DecoderRNN```. In order to do this, get the vocabulary sizes for the appropriate languages from the ```TranslationDataset``` object.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295612e6",
      "metadata": {
        "id": "295612e6"
      },
      "outputs": [],
      "source": [
        "# Get the vocabulary sizes for the appropriate languages from the TranslationDataset object\n",
        "input_size = len(new_dataset.input_lang.word2idx)\n",
        "output_size = len(new_dataset.output_lang.word2idx)\n",
        "\n",
        "encoder = EncoderRNN(input_size=input_size, hidden_size=hidden_size).to(device)\n",
        "decoder = DecoderRNN(hidden_size=hidden_size, output_size=output_size).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3ae2b3e",
      "metadata": {
        "id": "f3ae2b3e"
      },
      "source": [
        "Implement the training loop into the ```train_epoch``` function. Follow the model from the previous lab. Note that we will use separated *optimizers* for the encoder and decoder. **Be careful to the sizes of the model outputs for use with the criterion !**\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec7ef47a",
      "metadata": {
        "id": "ec7ef47a"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        target_tensor = target_tensor.to(device)\n",
        "        # Initiate gradient\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        # Forward\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor.size(1), target_tensor)\n",
        "        # Compute loss : put the output at the right size, the reference too, and apply the criterion\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        # Compute gradient\n",
        "        loss.backward()\n",
        "        # Update weights\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "        # Keep track of loss\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5324307",
      "metadata": {
        "id": "f5324307"
      },
      "source": [
        "We can know simply loop on this using the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0a14bc",
      "metadata": {
        "id": "3c0a14bc"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs=80, learning_rate=0.001, print_every=10, plot_every=10):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    # Initialize optimizers\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    # Initialize criterion\n",
        "    criterion = nn.NLLLoss()\n",
        "    # Training loop\n",
        "    for epoch in range(n_epochs):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if (epoch + 1) % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('(%d %d%%) %.4f' % (epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if (epoch + 1) % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6638f457",
      "metadata": {
        "id": "6638f457"
      },
      "source": [
        "And we need to also implement an ```evaluate``` function. Here, we will need to use the decoder in **inference** node, so it will re-use what output it generates to continue processing. We will then transform this sequence of outputs into **words**. What is the stopping condition for our model generating words ?\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb3e0cf",
      "metadata": {
        "id": "9bb3e0cf"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length, input_lang, output_lang):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    # One example to evaluate\n",
        "    # We need to make it into a batch of one exemple to respect tensor dimensions\n",
        "    input_tensor = tensorFromSentence(sentence, input_lang, max_length).view(1, -1).to(device)\n",
        "    # Forward\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "    decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden, max_length)\n",
        "    # Get best output\n",
        "    _, topi = decoder_outputs.topk(1)\n",
        "    decoded_ids = topi.squeeze()\n",
        "    # Decode until stopping condition ?\n",
        "    decoded_words = []\n",
        "    for idx in decoded_ids:\n",
        "        if idx.item() == EOS_TOKEN:\n",
        "            decoded_words.append('<EOS>')\n",
        "            break\n",
        "        decoded_words.append(output_lang.idx2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ebafa5",
      "metadata": {
        "id": "23ebafa5"
      },
      "source": [
        "Let's use this function to evaluate our model on a random subset of the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3236f826",
      "metadata": {
        "id": "3236f826"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, dataset, n=10):\n",
        "    # do n examples\n",
        "    for i in range(n):\n",
        "        # select one from the known data to avoid vocabulary issue\n",
        "        pair = random.choice(dataset.pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], dataset.max_length, dataset.input_lang, dataset.output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d57249c",
      "metadata": {
        "id": "3d57249c"
      },
      "source": [
        "Now, execute the training loop for, and look at what it generates. It should be fast on a cpu, and not take too long on a GPU.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a6ea9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3a6ea9f",
        "outputId": "c4fa73dc-ac20-4dc8-f579-7b9315aa8180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4 5%) 1.6361\n",
            "(9 11%) 1.0442\n",
            "(14 17%) 0.7959\n",
            "(19 23%) 0.6289\n",
            "(24 30%) 0.5072\n",
            "(29 36%) 0.4142\n",
            "(34 42%) 0.3445\n",
            "(39 48%) 0.2915\n",
            "(44 55%) 0.2494\n",
            "(49 61%) 0.2149\n",
            "(54 67%) 0.1874\n",
            "(59 73%) 0.1648\n",
            "(64 80%) 0.1463\n",
            "(69 86%) 0.1279\n",
            "(74 92%) 0.1140\n",
            "(79 98%) 0.1013\n"
          ]
        }
      ],
      "source": [
        "train(training_dataloader, encoder, decoder, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder, decoder, new_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmJC55dwdXqi",
        "outputId": "db6996f5-2a64-4a37-b961-bfeee4d6e288"
      },
      "id": "MmJC55dwdXqi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> i am ready to follow you\n",
            "= je suis prêt à te suivre\n",
            "< <EOS>\n",
            "\n",
            "> she is a nurse\n",
            "= elle est infirmière\n",
            "< SOS c'est un bébé <EOS>\n",
            "\n",
            "> he is almost six feet tall\n",
            "= il fait presque six pieds de haut\n",
            "< SOS il est sûr de réussir l'examen <EOS>\n",
            "\n",
            "> i am afraid it will rain in the afternoon\n",
            "= je crains qu'il ne pleuve dans l'aprèsmidi\n",
            "< j'ai amitié de tête de tête de café <EOS>\n",
            "\n",
            "> i am interested in swimming\n",
            "= la natation m'intéresse\n",
            "< la dernière personne la plus la tête plus un de\n",
            "\n",
            "> she is no match for me\n",
            "= elle ne fait pas le poids avec moi\n",
            "< SOS elle n'a pas très riche mais elle est heureuse\n",
            "\n",
            "> we are the same age but different heights\n",
            "= nous avons le même âge mais sommes de tailles différentes\n",
            "< minute la tête de la ville <EOS>\n",
            "\n",
            "> he is engaged to my younger sister\n",
            "= il est fiancé à ma jeune sur\n",
            "< SOS il est trop honnête pour dire à la dernière\n",
            "\n",
            "> i am growing to hate the girl\n",
            "= je me mets à détester cette fille\n",
            "< <EOS>\n",
            "\n",
            "> you aren't as short as i am\n",
            "= vous n'êtes pas aussi petit que moi\n",
            "< <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4183c681",
      "metadata": {
        "id": "4183c681"
      },
      "source": [
        "### II - Attention module\n",
        "\n",
        "We will know implement a new class ```Attention``` inheriting from ```Module```.\n",
        "\n",
        "Begin by implementing it following the scheme presented in class. In order to implement this efficiently, you will need to use *batched* operations and pay attention to shapes. in particular, use the **batched matrix multiplication** ```torch.bmm```. Use shape manipulation functions (```permute, squeeze, unsqueeze```) when needed.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba6303c2",
      "metadata": {
        "id": "ba6303c2"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        # What shape do we need the query in ?\n",
        "        query = query.squeeze(2)\n",
        "        print(\"\\nquery shape is: {}\".format(query.shape))\n",
        "        # Compute similarity scores with bmm. Any shape change for keys ?\n",
        "        keys = keys.transpose(1, 2)\n",
        "        print(\"\\nkeys shape is: {}\".format(keys.shape))\n",
        "        scores = torch.bmm(query, keys)\n",
        "        print(\"\\nscores shape is: {}\".format(scores.shape))\n",
        "        # Apply softmax to get weights. Any shape change for scores ?\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        # Use bmm to make weighted sum. Any shape change required ?\n",
        "        context = torch.bmm(weights, keys)\n",
        "        context = context.squeeze(1)\n",
        "        return context, weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca8cb3c",
      "metadata": {
        "id": "3ca8cb3c"
      },
      "source": [
        "Then, you will need to modify the decoder class into a new ```AttentionDecoderRNN```. The usual way of implementing the loop in ```forward_step``` is as follows:\n",
        "\n",
        "- Apply the recurrent loop as before: $\\mathbf{s}_{t} = \\text{GRU}(\\mathbf{r}_t, \\mathbf{s}_{t-1})$\n",
        "- Noting $\\mathbf{z}_t = Attention(\\mathbf{H}, \\mathbf{s}_t)$ the output of the attention, we compute a modified state $\\tilde{s}_t$: $$ \\tilde{s}_t = tanh(\\mathbf{W}_a \\times [\\mathbf{z}_t; \\mathbf{s}_t])$$ based on the concatenation of the attention output and output of the GRU.\n",
        "- We predict score based on this modified new state: $\\mathbf{o}_t = \\mathbf{W}_{out} \\times \\tilde{s}_t$.\n",
        "\n",
        "**Important**:\n",
        "- You need to instantiate the ```Attention``` class when building the decoder.\n",
        "- You also need a new parameter representing $\\mathbf{W}_a$, of the appropriate size - as this matrix is applied to a concatenation of the attention output and the decoder hidden state.\n",
        "- You need to keep track of attention weights at each step, and also concatenate them and output them at the end of the ```forward```.\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttentionDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        # Don't forget to instantiate the Attention\n",
        "        self.attention = Attention(hidden_size=hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        # And the new linear layer needed\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        # Get your input through embedding, apply the recurrent layer\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        gru_output, hidden = self.gru(embedded, hidden)\n",
        "        # Compute the attention\n",
        "        query = hidden\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        # Concatenate the result of the attention and the encoder outputs\n",
        "        concat_input = torch.cat((gru_output, context), dim=2)\n",
        "        # Apply the linear transformation and tanh\n",
        "        output, hidden = torch.tanh(self.concat(concat_input))\n",
        "        # Apply the last layer to obtain scores\n",
        "        output = self.out(output)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, max_length, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, dtype=torch.long, device=device).fill_(SOS_TOKEN)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        # New: attention list\n",
        "        attention_weights = []\n",
        "\n",
        "        for i in range(max_length):\n",
        "            decoder_output, decoder_hidden, weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            # Also keep track of attentions\n",
        "            attention_weights.append(weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
        "\n",
        "            else:\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, attention_weights"
      ],
      "metadata": {
        "id": "0DTetsUbrH2U"
      },
      "id": "0DTetsUbrH2U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3d1e4b35",
      "metadata": {
        "id": "3d1e4b35"
      },
      "source": [
        "Create new encoder and decoders instances for this model, and train them !\n",
        "\n",
        "<div class='alert alert-block alert-info'>\n",
        "            Code:</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c36774c7",
      "metadata": {
        "id": "c36774c7"
      },
      "outputs": [],
      "source": [
        "encoder_att = EncoderRNN(input_size=input_size, hidden_size=hidden_size).to(device)\n",
        "decoder_att = AttentionDecoderRNN(hidden_size=hidden_size, output_size=output_size).to(device)\n",
        "\n",
        "train(training_dataloader, encoder_att, decoder_att, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tried various methods to implement the attentionDecoder but unfortunately an error concerning the tensors size has persisted. If it is possible we request a correction for this lab or maybe some tips about how we can resolve this issue as we are interested to learn more and deepen our knowledge in NLP.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Sss3JKErQCOQ"
      },
      "id": "Sss3JKErQCOQ"
    },
    {
      "cell_type": "markdown",
      "id": "a75d6278",
      "metadata": {
        "id": "a75d6278"
      },
      "source": [
        "Use the following function to visualize the attention learnt by the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ffe06a3",
      "metadata": {
        "id": "7ffe06a3"
      },
      "outputs": [],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "    plt.show()\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence, encoder, decoder, dataset):\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_sentence, dataset.max_length, dataset.input_lang, dataset.output_lang)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c0ddb1f",
      "metadata": {
        "id": "0c0ddb1f"
      },
      "outputs": [],
      "source": [
        "evaluateRandomly(encoder_att, decoder_att, training_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c992013",
      "metadata": {
        "id": "3c992013"
      },
      "outputs": [],
      "source": [
        "evaluateAndShowAttention('i am not a doctor but a teacher', encoder_att, decoder_att, training_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03b0c870",
      "metadata": {
        "id": "03b0c870"
      },
      "source": [
        "We played here with a dataset but did not rigorously evaluate. If you were to create a model or re-use one, describe the rigorous methodology you could use to **evaluate model performance** - look for appropriate metrics and which functions you could use.\n",
        "\n",
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our lab exercise, we did not divide the dataset into distinct sets for training and testing. Therefore, when creating a model or re-using one, for starters I should split the dataset to training, validation and testing. the test subset is used to evaluate the model, while the validation subset is used to fine-tune its hyperparameters.\n",
        "For the purpose of evaluating the model, various metrics can be used including the metric that we've seen in class: ***BLEU (Bilingual Evaluation Understudy)***. BLEU can is the best choice to be used thanks to its ability to measure how many words and phrases in the translated output match reference translations.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TFvIuz7KIu3D"
      },
      "id": "TFvIuz7KIu3D"
    },
    {
      "cell_type": "markdown",
      "id": "14b956da",
      "metadata": {
        "id": "14b956da"
      },
      "source": [
        "We improved our initial model with attention. But considering our goal is to **generate text**, what is the aspect we did not consider yet and that we could improve ? What did we do 'too simply' and with which algorithm could we improve it ? How would you go about implementing that given our current code ?\n",
        "\n",
        "<div class='alert alert-block alert-warning'>\n",
        "            Question:</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we aim to generate text, an essential aspect that could be further refined is the use of transformers thanks to their ability to handle sequences by enabeling parallel processing and capturing long-range dependencies more effectively than the RNN-based models we've been using in this lab.\n",
        "In this seq2seq project we implemented a simple attention mechanism in the decoder which is relatively too simple, therefore we could have a potential improvement using a more complex approach: the ***self-attention mechanism*** which allows each part of the input sequence to attend to every other part.\n",
        "To do so we could replace the RNN-based encoder and decoder with Transformer blocks that use self-attention: including multi-head self-attention layers and positional encoding.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WTQqim-NL83q"
      },
      "id": "WTQqim-NL83q"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p0p4qVIFNiau"
      },
      "id": "p0p4qVIFNiau",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}